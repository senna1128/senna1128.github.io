[{"authors":["mihai-anitescu"],"categories":null,"content":"Mihai Anitescu is a Professor in the Statistics and CAM departments at the University of Chicago, and is also a senior computational mathematician in the Mathematics and Computer Science Division at Argonne. He works on a variety of topics on control, optimization, and computational statistics. » Personal website\n","date":1718144108,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1698624e3,"objectID":"a6234a836d542de2f7c07553d03a7206","permalink":"https://senna1128.github.io/authors/mihai-anitescu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mihai-anitescu/","section":"authors","summary":"Mihai Anitescu is a Professor in the Statistics and CAM departments at the University of Chicago, and is also a senior computational mathematician in the Mathematics and Computer Science Division at Argonne.","tags":null,"title":"Mihai Anitescu","type":"authors"},{"authors":["sen-na"],"categories":null,"content":"I am currently an Assistant Professor in the H. Milton Stewart School of Industrial and Systems Engineering (ISyE) at Georgia Tech. Prior to joining ISyE, I was a postdoctoral researcher in the Department of Statistics and International Computer Science Institute (ICSI) at UC Berkeley, working with Michael W. Mahoney.\nI received my Ph.D. degree in statistics from the University of Chicago in 2021, under the supervision of Mihai Anitescu and Mladen Kolar. Before attending UChicago, I received my B.S. degree in mathematics from Nanjing University in 2016. I also visited the Statistics department at UC Davis as an exchange student and worked with Jane-Ling Wang, Hans-Georg Müller, and Christiana Drake in 2015.\nI am broadly interested in the mathematical foundations of data science, with topics including high-dimensional statistics, graphical models, semiparametric models, optimal control, and large-scale and stochastic nonlinear optimization. I am also interested in the applications of machine learning methods in biology, neuroscience, and engineering.\n(I’m actively seeking graduate students who are interested in interdisciplinary research in statistics, optimization, and data science. * GT students can send me an email with CV and transcript. * Prospective students should apply directly to our PhD or Master’s programs and mention my name in the application.)\n","date":1718144108,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1698624e3,"objectID":"0a7e3948f19b96cd96eb3d42cbc5e019","permalink":"https://senna1128.github.io/authors/sen-na/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sen-na/","section":"authors","summary":"I am currently an Assistant Professor in the H. Milton Stewart School of Industrial and Systems Engineering (ISyE) at Georgia Tech. Prior to joining ISyE, I was a postdoctoral researcher in the Department of Statistics and International Computer Science Institute (ICSI) at UC Berkeley, working with Michael W.","tags":null,"title":"Sen Na","type":"authors"},{"authors":["sungho-shin"],"categories":null,"content":"Sungho Shin is a postdoctoral researcher at the Mathematics and Computer Science Division at Argonne National Laboratory. He received his Ph.D. from the University of Wisconsin-Madison in 2021. He was a Summer intern at Los Alamos National Laboratory and Argonne National Laboratory. He was an undergraduate researcher at Jong Min Lee’s group at Seoul National University.\nHis research interests include model predictive control, optimization algorithms, and their applications to large-scale energy infrastructures (such as natural gas and power networks). He is the main developer of nonlinear optimization solver MadNLP.jl and automatic differentiation/algebraic modeling tool MadDiff.jl. He was the winner of the 2020 AIChE Annual Meeting CAST Directors’ Student Presentation Award, 2021 IFAC ADCHEM Young Author Award, and 2021 IFAC NMPC Young Author Award. He was a recipient of the Korea Presidential Science Fellowship, Kwanjeong Fellowship, and Grainger Wisconsin Distinguished Graduate Fellowship. » Personal website\n","date":1718144108,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1677652142,"objectID":"7ae2ba3ba44fbc599fc5da13ea6c5176","permalink":"https://senna1128.github.io/authors/sungho-shin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sungho-shin/","section":"authors","summary":"Sungho Shin is a postdoctoral researcher at the Mathematics and Computer Science Division at Argonne National Laboratory. He received his Ph.D. from the University of Wisconsin-Madison in 2021. He was a Summer intern at Los Alamos National Laboratory and Argonne National Laboratory.","tags":null,"title":"Sungho Shin","type":"authors"},{"authors":["michael-w.-mahoney"],"categories":null,"content":"Michael Mahoney is in the Department of Statistics at UC Berkeley. He is also at the International Computer Science Institute (ICSI, where he is Vice President and Director of the Big Data Group) and the Lawrence Berkeley National Laboratory (LBNL, where he is the Group Lead for the Machine Learning and Analytics Group). He is also in the RISELab (in the past AMPLab) in the Department of EECS, and he is an Amazon Scholar. » Personal website\n","date":1706486400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1698624e3,"objectID":"ae50e44af4a0c8b2ebe03c0b1ba428f7","permalink":"https://senna1128.github.io/authors/michael-w.-mahoney/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/michael-w.-mahoney/","section":"authors","summary":"Michael Mahoney is in the Department of Statistics at UC Berkeley. He is also at the International Computer Science Institute (ICSI, where he is Vice President and Director of the Big Data Group) and the Lawrence Berkeley National Laboratory (LBNL, where he is the Group Lead for the Machine Learning and Analytics Group).","tags":null,"title":"Michael W. Mahoney","type":"authors"},{"authors":["mladen-kolar"],"categories":null,"content":"Mladen Kolar is an Associate Professor of Econometrics and Statistics at the University of Chicago Booth School of Business. His research is focused on high-dimensional statistical methods, graphical models, varying-coefficient models and data mining, driven by the need to uncover interesting and scientifically meaningful structures from observational data. » Personal website\n","date":1706486400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1698624e3,"objectID":"fb77a1af752219b5419189adcc2effb6","permalink":"https://senna1128.github.io/authors/mladen-kolar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mladen-kolar/","section":"authors","summary":"Mladen Kolar is an Associate Professor of Econometrics and Statistics at the University of Chicago Booth School of Business. His research is focused on high-dimensional statistical methods, graphical models, varying-coefficient models and data mining, driven by the need to uncover interesting and scientifically meaningful structures from observational data.","tags":null,"title":"Mladen Kolar","type":"authors"},{"authors":["yuchen-fang"],"categories":null,"content":"Yuchen Fang is a PhD student in the mathematics department at UC Berkeley. He was a master student in the Computational and Applied Math program at UChicago, where he worked with Mladen Kolar and Sen Na on stochastic nonlinear optimization. His research interests include numerical linear algebra, high-dimensional statistics, statistical learning, and mathematical finance. » Personal website\n","date":1706486400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1677622508,"objectID":"a5a3fd264bd5d70583f3df578c430813","permalink":"https://senna1128.github.io/authors/yuchen-fang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuchen-fang/","section":"authors","summary":"Yuchen Fang is a PhD student in the mathematics department at UC Berkeley. He was a master student in the Computational and Applied Math program at UChicago, where he worked with Mladen Kolar and Sen Na on stochastic nonlinear optimization.","tags":null,"title":"Yuchen Fang","type":"authors"},{"authors":["wei-kuang"],"categories":null,"content":"Wei Kuang is currently a PhD student in the Statistics department at UChicago, working with Mihai Anitescu (supervisor) and Sen Na on randomized second-order methods, with an emphasis on the uncertainty quantification and statistical inference aspects.\n","date":1698624e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1698624e3,"objectID":"6d0f47015abbccbdf730508824f14c4a","permalink":"https://senna1128.github.io/authors/wei-kuang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/wei-kuang/","section":"authors","summary":"Wei Kuang is currently a PhD student in the Statistics department at UChicago, working with Mihai Anitescu (supervisor) and Sen Na on randomized second-order methods, with an emphasis on the uncertainty quantification and statistical inference aspects.","tags":null,"title":"Wei Kuang","type":"authors"},{"authors":["you-lin-chen"],"categories":null,"content":"You-Lin Chen was a statistics PhD candidate at the University of Chicago. He pursues his research interests in machine learning, stochastic and non-convex optimization, high-dimensional statistics.\nPersonal webpage\n","date":1698624e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1698624e3,"objectID":"a079d4997047a42cd56619f25e7c082f","permalink":"https://senna1128.github.io/authors/you-lin-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/you-lin-chen/","section":"authors","summary":"You-Lin Chen was a statistics PhD candidate at the University of Chicago. He pursues his research interests in machine learning, stochastic and non-convex optimization, high-dimensional statistics.\nPersonal webpage","tags":null,"title":"You-Lin Chen","type":"authors"},{"authors":["ilgee-hong"],"categories":null,"content":"Ilgee Hong is a master student in the Statistics department at UChicago, working with Mladen Kolar and Sen Na on randomized numerical methods, with an emphasis on machine learning applications. » Personal website\n","date":1685491200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1685491200,"objectID":"0aed101c6cc00f4dbda0c06ae780a11f","permalink":"https://senna1128.github.io/authors/ilgee-hong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ilgee-hong/","section":"authors","summary":"Ilgee Hong is a master student in the Statistics department at UChicago, working with Mladen Kolar and Sen Na on randomized numerical methods, with an emphasis on machine learning applications. » Personal website","tags":null,"title":"Ilgee Hong","type":"authors"},{"authors":["miao-li"],"categories":null,"content":"Miao Li was a master student in the Computational and Applied Math program at UChicago, working with Mladen Kolar and Sen Na on manifold optimization, with applications on matrix completion problems.\n","date":1677369600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1677369600,"objectID":"866e4600723515b6bdbce8e751cea110","permalink":"https://senna1128.github.io/authors/miao-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/miao-li/","section":"authors","summary":"Miao Li was a master student in the Computational and Applied Math program at UChicago, working with Mladen Kolar and Sen Na on manifold optimization, with applications on matrix completion problems.","tags":null,"title":"Miao Li","type":"authors"},{"authors":["michał-dereziński"],"categories":null,"content":"Michał Dereziński is an Assistant Professor of Computer Science and Engineering at the University of Michigan. He is interested in the foundations of machine learning and optimization, randomized linear algebra, high-dimensional statistics, and random matrix theory. » Personal website\n","date":1669852800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1677622507,"objectID":"9b2a291aa3daf107f61d2f090532afe4","permalink":"https://senna1128.github.io/authors/micha%C5%82-derezinski/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/micha%C5%82-derezinski/","section":"authors","summary":"Michał Dereziński is an Assistant Professor of Computer Science and Engineering at the University of Michigan. He is interested in the foundations of machine learning and optimization, randomized linear algebra, high-dimensional statistics, and random matrix theory.","tags":null,"title":"Michał Dereziński","type":"authors"},{"authors":["victor-m.-zavala"],"categories":null,"content":"Victor M. Zavala is a Baldovin-DaPra Professor of Chemical and Biological Engineering at the University of Wisconsin at Madison. His group focuses on the use of mathematics and computation to tackle diverse problems arising in optimization, control, data science, and energy/environmental systems. » Personal website\n","date":1667260800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1677652142,"objectID":"4f488578b4e90c88673708715da5a367","permalink":"https://senna1128.github.io/authors/victor-m.-zavala/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/victor-m.-zavala/","section":"authors","summary":"Victor M. Zavala is a Baldovin-DaPra Professor of Chemical and Biological Engineering at the University of Wisconsin at Madison. His group focuses on the use of mathematics and computation to tackle diverse problems arising in optimization, control, data science, and energy/environmental systems.","tags":null,"title":"Victor M. Zavala","type":"authors"},{"authors":["mingyuan-ma"],"categories":null,"content":"Mingyuan Ma is a postdoc in the School of Computer Science at Peking University. He received his B.S. degree in mathematics from Nanjing University, China, and Ph.D. degree in computer science from Peking University, China. His research interests lie in graph and combinatorial optimization, neural networks, and biological computing. » Personal website\n","date":1659312e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1677706319,"objectID":"02e540085f7766b23cfa603dda7fe049","permalink":"https://senna1128.github.io/authors/mingyuan-ma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mingyuan-ma/","section":"authors","summary":"Mingyuan Ma is a postdoc in the School of Computer Science at Peking University. He received his B.S. degree in mathematics from Nanjing University, China, and Ph.D. degree in computer science from Peking University, China.","tags":null,"title":"Mingyuan Ma","type":"authors"},{"authors":["oluwasanmi-koyejo"],"categories":null,"content":"Sanmi Koyejo is an Assistant Professor in the Department of Computer Science at Stanford University. Koyejo’s research interests are in developing the principles and practice of trustworthy machine learning. Additionally, Koyejo focuses on applications to neuroscience and healthcare. Koyejo has been the recipient of several awards, including a best paper award from the conference on uncertainty in artificial intelligence (UAI), a Skip Ellis Early Career Award, and a Sloan Fellowship. » Personal website\n","date":1598918400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1677622506,"objectID":"c577dead1c2bf8060ab7f593a6b44ae0","permalink":"https://senna1128.github.io/authors/oluwasanmi-koyejo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/oluwasanmi-koyejo/","section":"authors","summary":"Sanmi Koyejo is an Assistant Professor in the Department of Computer Science at Stanford University. Koyejo’s research interests are in developing the principles and practice of trustworthy machine learning. Additionally, Koyejo focuses on applications to neuroscience and healthcare.","tags":null,"title":"Oluwasanmi Koyejo","type":"authors"},{"authors":["yuwei-luo"],"categories":null,"content":"Yuwei Luo received his M.S. degree in Statistics at University of Chicago in March 2020. Prior to graduate school, he received B.S.degree in Mathematics at University of Science and Technology of China (USTC) in June 2018. His research interests include reinforcement learning, control, optimization and network analysis. Yuwei is continuing his education as a PhD student at Stanford University.\n» Personal website\n","date":1588291200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1677622507,"objectID":"8e00ad7d6795204b9b4e9f8b9665c19e","permalink":"https://senna1128.github.io/authors/yuwei-luo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuwei-luo/","section":"authors","summary":"Yuwei Luo received his M.S. degree in Statistics at University of Chicago in March 2020. Prior to graduate school, he received B.S.degree in Mathematics at University of Science and Technology of China (USTC) in June 2018.","tags":null,"title":"Yuwei Luo","type":"authors"},{"authors":["zhaoran-wang"],"categories":null,"content":"Zhaoran Wang is an Assistant Professor in the Departments of Industrial Engineering \u0026amp; Management Sciences and Computer Science at Northwestern University. His research is to develop a new generation of data-driven decision-making methods, theory, and systems, which tailor artificial intelligence towards addressing pressing societal challenges. » Personal website\n","date":1588291200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1677622507,"objectID":"e9caf695c481b504fb7491cd5dd216ef","permalink":"https://senna1128.github.io/authors/zhaoran-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhaoran-wang/","section":"authors","summary":"Zhaoran Wang is an Assistant Professor in the Departments of Industrial Engineering \u0026 Management Sciences and Computer Science at Northwestern University. His research is to develop a new generation of data-driven decision-making methods, theory, and systems, which tailor artificial intelligence towards addressing pressing societal challenges.","tags":null,"title":"Zhaoran Wang","type":"authors"},{"authors":["zhuoran-yang"],"categories":null,"content":"Zhuoran Yang is an Assistant Professor of Statistics and Data Science at Yale University. His research interests lie in the interface between machine learning, statistics and optimization. The primary goal of his research is to design efficient learning algorithms for large-scale decision making problems that arise in reinforcement learning and stochastic games, with both statistical and computational guarantees. » Personal website\n","date":1588291200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1677622507,"objectID":"20ebad352f9ba9ebdf682847ef1ef7d9","permalink":"https://senna1128.github.io/authors/zhuoran-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhuoran-yang/","section":"authors","summary":"Zhuoran Yang is an Assistant Professor of Statistics and Data Science at Yale University. His research interests lie in the interface between machine learning, statistics and optimization. The primary goal of his research is to design efficient learning algorithms for large-scale decision making problems that arise in reinforcement learning and stochastic games, with both statistical and computational guarantees.","tags":null,"title":"Zhuoran Yang","type":"authors"},{"authors":["Runxin Ni","Sen Na","Sungho Shin","Mihai Anitescu"],"categories":[],"content":"","date":1718144108,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677622508,"objectID":"89073c53f0a13334f3ce1c802af4652a","permalink":"https://senna1128.github.io/publication/preprints/ni-2024-distributed/","publishdate":"2024-06-11T22:15:08.368479Z","relpermalink":"/publication/preprints/ni-2024-distributed/","section":"publication","summary":"In this paper, we address the challenge of solving **large-scale graph-structured nonlinear programs (gsNLPs)** in a scalable manner. GsNLPs are problems in which the objective and constraint functions are associated with nodes on a graph and depend on the variables of adjacent nodes. This graph-structured formulation encompasses various specific instances, such as dynamic optimization, PDE-constrained optimization, multistage stochastic optimization, and general network optimization. By leveraging the sequential quadratic programming (SQP) framework, we propose a globally convergent overlapping graph decomposition method to solve large-scale gsNLPs under standard mild regularity conditions on the graph topology. In each iteration, we perform an overlapping graph decomposition to compute an approximate Newton direction in a parallel environment. Then, we select a suitable stepsize and update the primal-dual iterate by performing a backtracking line search on an exact augmented Lagrangian merit function. Built on the **exponential decay of sensitivity** of gsNLPs, we show that the approximate Newton direction is a descent direction of the augmented Lagrangian, which leads to global convergence with local linear convergence rate. In particular, global convergence is achieved for sufficiently large overlaps, and the local linear convergence rate improves exponentially in terms of the overlap size. Our results match existing state-of-the-art guarantees established for dynamic programs (which simply correspond to linear graphs). We validate the theory on a semilinear elliptic PDE-constrained problem. ","tags":["math.OC"],"title":"Distributed Sequential Quadratic Programming with Overlapping Graph Decomposition and Exact Augmented Lagrangian","type":"publication"},{"authors":["Yuchen Fang","Sen Na","Michael W. Mahoney","Mladen Kolar"],"categories":[],"content":"","date":1706486400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677622508,"objectID":"f403087b036336245d7523d81f9fbf10","permalink":"https://senna1128.github.io/publication/pubs/fang-2024-fully/","publishdate":"2023-02-28T22:15:08.487247Z","relpermalink":"/publication/pubs/fang-2024-fully/","section":"publication","summary":"We propose a trust-region stochastic sequential quadratic programming algorithm (TR-StoSQP) to solve nonlinear optimization problems with stochastic objectives and deterministic equality constraints. We consider a fully stochastic setting, where at each step a single sample is generated to estimate the objective gradient. The algorithm adaptively selects the trust-region radius and, compared to the existing line-search StoSQP schemes, allows us to utilize indefinite Hessian matrices (i.e., Hessians without modification) in SQP subproblems. As a trust-region method for constrained optimization, our algorithm must address an infeasibility issue --- the linearized equality constraints and trust-region constraints may lead to infeasible SQP subproblems. In this regard, we propose an **adaptive relaxation technique** to compute the trial step, consisting of a normal step and a tangential step. To control the lengths of these two steps while ensuring a **scale-invariant property**, we adaptively decompose the trust-region radius into two segments, based on the proportions of the rescaled feasibility and optimality residuals to the rescaled full KKT residual. The normal step has a closed form, while the tangential step is obtained by solving a trust-region subproblem, to which a solution ensuring the Cauchy reduction is sufficient for our study. We establish a global almost sure convergence guarantee for TR-StoSQP, and illustrate its empirical performance on both a subset of problems in the CUTEst test set and constrained logistic regression problems using data from the LIBSVM collection.","tags":["math.OC","stat.CO","stat.ML","recent"],"title":"Fully Stochastic Trust-Region Sequential Quadratic Programming for Equality-Constrained Optimization Problems","type":"publication"},{"authors":["Sen Na","Yihang Gao","Michael K. Ng","Michael W. Mahoney"],"categories":[],"content":"","date":1706313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677622508,"objectID":"0a1b6a21fdb479dbe9e1c93c98dffa65","permalink":"https://senna1128.github.io/publication/preprints/na-2024-asymptotically/","publishdate":"2023-02-28T22:15:08.487247Z","relpermalink":"/publication/preprints/na-2024-asymptotically/","section":"publication","summary":" We perform statistical inference for the solution of stochastic optimization problems with equality and box inequality constraints. The considered problems are prevalent in statistics and machine learning, encompassing constrained $M$-estimation, PDE-constrained problems, physics-inspired networks, and algorithmic fairness. We introduce a stochastic sequential quadratic programming method (StoSQP) to solve these problems, where we determine the search direction by performing a quadratic approximation of the objective with a linear approximation of the constraints. Despite having access to unbiased estimates of population gradients, a key challenge in constrained problems lies in dealing with the bias in the search direction. To address this challenge, we introduce a novel **gradient averaging technique to debias the direction step**, leading to Debiased-StoSQP. Our method achieves **global almost sure convergence** and exhibits **local asymptotic normality** with an optimal limiting covariance matrix in Hájek and Le Cam's sense. Additionally, a plug-in covariance matrix estimator is provided for practical inference purposes. To our knowledge, Debiased-StoSQP is the first **fully online** method to achieve **asymptotic minimax optimality** without relying on projection operators to the constraint set, which are incomputable for nonlinear problems. Through extensive experiments on benchmark nonlinear problems in the CUTEst test set, as well as on constrained generalized linear models and portfolio allocation problems, with both synthetic and real data, we demonstrate the superior performance of the method. ","tags":["math.OC","stat.CO","stat.ML"],"title":"An Asymptotically Optimal Method for Constrained Stochastic Optimization","type":"publication"},{"authors":["You-Lin Chen","Sen Na","Mladen Kolar"],"categories":[],"content":"","date":1698624e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698624e3,"objectID":"a3e6d3558f3d5135a8ed24d202dbb08b","permalink":"https://senna1128.github.io/publication/pubs/chen-2023-convergence/","publishdate":"2023-10-28T22:15:07.525235Z","relpermalink":"/publication/pubs/chen-2023-convergence/","section":"publication","summary":" We study the convergence of accelerated stochastic gradient descent for strongly convex objectives under the **growth condition**, which states that the variance of stochastic gradient is bounded by a multiplicative part that grows with the full gradient, and a constant additive part. Through the lens of the growth condition, we investigate four widely used accelerated methods: **Nesterov's accelerated method** (NAM), **robust momentum method** (RMM), **accelerated dual averaging method** (DAM+), and **implicit DAM+** (iDAM+). While these methods are known to improve the convergence rate of SGD under the condition that the stochastic gradient has bounded variance, it is not well understood how their convergence rates are affected by the multiplicative noise. In this paper, we show that these methods all converge to a neighborhood of the optimum with accelerated convergence rates (compared to SGD) even under the growth condition. In particular, NAM, RMM, iDAM+ enjoy acceleration only with a mild multiplicative noise, while DAM+ enjoys acceleration even with a large multiplicative noise. Furthermore, we propose a generic tail-averaged scheme that allows the accelerated rates of DAM+ and iDAM+ to nearly attain the theoretical lower bound (up to a logarithmic factor in the variance term). We conduct numerical experiments to support our theoretical conclusions. ","tags":["math.OC","recent"],"title":"Convergence Analysis of Accelerated Stochastic Gradient Descent under the Growth Condition","type":"publication"},{"authors":["Wei Kuang","Sen Na","Michael W. Mahoney","Mihai Anitescu"],"categories":null,"content":"","date":1698624e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698624e3,"objectID":"ef79506dddf20926be3dd6b3ddf59a8b","permalink":"https://senna1128.github.io/publication/ongoing/kuang-2023-online/","publishdate":"2023-10-30T00:00:00Z","relpermalink":"/publication/ongoing/kuang-2023-online/","section":"publication","summary":" Online algorithms gain prominence as the volume of data explodes, among which second-order methods are known for their robustness. While much research focuses on the convergence of stochastic second-order methods, the algorithms' variability remains underexplored. This paper studies the statistical inference of the estimates generated by an inexact adaptive stochastic Newton method. The algorithm not only allows adaptive stepsize, but also significantly reduces computational costs by inexactly solving the Newton systems with a randomized solver involving sketching techniques. For the designed algorithm, we establish the asymptotic normality of its last iterate and, more importantly, construct a fully online limiting covariance estimator. Our covariance estimator solely utilizes the iterates (with varying weights) from the stochastic Newton algorithm, which outperforms the plug-in estimator in terms of both consistency and computational efficiency. We establish the convergence rate of the weighted covariance estimator and illustrate its superior performance on regression problems. ","tags":["math.OC","cs.NA","math.NA","stat.CO","stat.ML"],"title":"Online Covariance Matrix Estimation in Stochastic Inexact Newton Methods","type":"publication"},{"authors":["Ilgee Hong","Sen Na","Michael W. Mahoney","Mladen Kolar"],"categories":null,"content":"","date":1685491200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685491200,"objectID":"cc0071d3313117e1351a3bb384c306c9","permalink":"https://senna1128.github.io/publication/pubs/hong-2023-constrained/","publishdate":"2023-05-31T00:00:00Z","relpermalink":"/publication/pubs/hong-2023-constrained/","section":"publication","summary":"We consider solving equality-constrained nonlinear, nonconvex optimization problems. This class of problems appears widely in a variety of applications in machine learning and engineering, ranging from constrained deep neural networks, to optimal control, to PDE-constrained optimization. We develop an adaptive inexact Newton method for this problem class. In each iteration, we solve the Lagrangian Newton system inexactly via a **randomized iterative sketching** solver, and select a suitable stepsize by performing line search on an **exact augmented Lagrangian** merit function. The randomized solvers have advantages over deterministic linear system solvers by significantly reducing per-iteration flops complexity and storage cost, when equipped with suitable sketching matrices. Our method adaptively controls the accuracy of the randomized solver and the penalty parameters of the exact augmented Lagrangian, to ensure that the inexact Newton direction is a descent direction of the exact augmented Lagrangian. This allows us to establish a **global almost sure convergence**. We also show that a unit stepsize is admissible locally, so that our method exhibits a **local linear convergence**. Furthermore, we prove that the linear convergence can be strengthened to **superlinear convergence** if we gradually sharpen the adaptive accuracy condition on the randomized solver. We demonstrate the superior performance of our method on benchmark nonlinear problems in CUTEst test set, constrained logistic regression with data from LIBSVM, and a PDE-constrained problem.","tags":["math.OC","cs.LG","cs.NA","math.NA","stat.ML"],"title":"Constrained Optimization via Exact Augmented Lagrangian and Randomized Iterative Sketching","type":"publication"},{"authors":["Sen Na","Mihai Anitescu","Mladen Kolar"],"categories":[],"content":"","date":1680825600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677622507,"objectID":"83c11675b94d7e0dab09d21a5a9c34e5","permalink":"https://senna1128.github.io/publication/pubs/na-2023-fast/","publishdate":"2023-02-28T22:15:07.753261Z","relpermalink":"/publication/pubs/na-2023-fast/","section":"publication","summary":"We propose a **fast** temporal decomposition procedure for solving long-horizon nonlinear dynamic programs. The core of the procedure is sequential quadratic programming (SQP) that utilizes a differentiable exact augmented Lagrangian as the merit function. Within each SQP iteration, we **approximately** solve the Newton system using an overlapping temporal decomposition strategy. We show that the approximate search direction is still a descent direction of the augmented Lagrangian, provided the overlap size and penalty parameters are suitably chosen, which allows us to establish the global convergence. Moreover, we show that a unit stepsize is accepted locally for the approximate search direction, and further establish a uniform, local linear convergence over stages. This local convergence rate matches the rate of the recent Schwarz scheme [(Na et al., 2022)](/publication/pubs/na-2022-convergence). However, the Schwarz scheme has to solve nonlinear subproblems to optimality in each iteration, while we only perform a single Newton step instead. Numerical experiments validate our theories and demonstrate the superiority of our method. ","tags":["math.OC","math.DS","recent"],"title":"A Fast Temporal Decomposition Procedure for Long-horizon Nonlinear Dynamic Programming","type":"publication"},{"authors":[],"categories":[],"content":"The above problem prominently appears in machine learning and statistics. For example, given $n$ response-feature data pairs $\\{(b_i,\\boldsymbol{a}_i)\\}_{i=1}^n$, we can let $\\mathcal{P} = \\text{Uniform}(\\{(b_i,\\boldsymbol{a}_i)\\}_{i=1}^n)$ be the empirical distribution. Then, $f$ reduces to the empirical loss $f(\\boldsymbol{x}) = \\frac{1}{n}\\sum_{i=1}^n F(\\boldsymbol{x}; b_i, \\boldsymbol{a}_i)$, and we arrive at the $M$-estimation problems. The constraints that encode prior model knowledge are ubiquitous in practice. For example, in semiparametric estimation, we require the true parameter $\\boldsymbol{x}^\\star$ to satisfy $\\boldsymbol{x}^\\star\\in\\{\\boldsymbol{x}\\in\\mathbb{R}^d:\\|\\boldsymbol{x}\\|^2 = 1, \\boldsymbol{x}_1\u0026gt;0\\}$ to resolve the identifiability issue (Na et al., 2019, Na and Kolar, 2021).\nCheck out my highlighted works Na et al., 2022, 2023, Fang et al., 2022!\n","date":1677801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677801600,"objectID":"0011edce4f15d80ed08da0c5dd14c1c3","permalink":"https://senna1128.github.io/project/current/constrainedopt/","publishdate":"2023-03-03T00:00:00Z","relpermalink":"/project/current/constrainedopt/","section":"project","summary":"The problems appear widely in *constrained M-estimation*, *semiparametric models*, and *constrained neural networks*.","tags":[],"title":"Deterministically Constrained Stochastic Optimization","type":"project"},{"authors":[],"categories":[],"content":"The first two plots show the histograms of the first-component error of the primal-dual iterates $\\{(\\boldsymbol{x}_t, \\boldsymbol{\\lambda}_t)\\}_t$ generated by AI-StoSQP, and the third plot shows the $95\\%$ confidence interval of $\\boldsymbol{x}^\\star_1+\\boldsymbol{\\lambda}^\\star_1$ constructed based on $\\{(\\boldsymbol{x}_t, \\boldsymbol{\\lambda}_t)\\}_t$. For more details, please refer to Na and Mahoney, 2022.\nWe denote $(\\boldsymbol{x}^\\star, \\boldsymbol{\\lambda}^\\star)$ as the primal-dual solution to a constrained problem with a population loss function. Statisticians aim to construct estimators based on $n$ samples and infer properties of $(\\boldsymbol{x}^\\star, \\boldsymbol{\\lambda}^\\star)$. Optimization people aim to design iterative stochastic approximation (SA) methods by realizing one sample at each step and demonstrate algorithmic convergence rates. Studying the limiting behavior of different SA methods bridges the gap between these two domains, enabling hypothesis testing and online construction of confidence intervals.\n","date":1677801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677801600,"objectID":"26bb673effb71e4d05a241ea3c65f680","permalink":"https://senna1128.github.io/project/current/statisticalinf/","publishdate":"2023-03-03T00:00:00Z","relpermalink":"/project/current/statisticalinf/","section":"project","summary":"We study the *stationarity* of SA methods, perform *online hypothesis testing*, and build *online confidence intervals*.","tags":[],"title":"Statistical Inference of Stochastic Approximation","type":"project"},{"authors":["Sen Na","Mihai Anitescu","Mladen Kolar"],"categories":[],"content":"","date":1677715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677785786,"objectID":"0c1cd5784d858f74153a4c7302de7744","permalink":"https://senna1128.github.io/publication/pubs/na-2023-inequality/","publishdate":"2023-03-02T19:36:25.79196Z","relpermalink":"/publication/pubs/na-2023-inequality/","section":"publication","summary":"We study nonlinear optimization problems with a stochastic objective and deterministic equality and inequality constraints, which emerge in numerous applications including finance, manufacturing, power systems and, recently, deep neural networks. We propose an active-set stochastic sequential quadratic programming (StoSQP) algorithm that utilizes a differentiable exact augmented Lagrangian as the merit function. The algorithm adaptively selects the penalty parameters of the augmented Lagrangian and performs a stochastic line search to decide the stepsize. The global convergence is established: for any initialization, the KKT residuals converge to zero almost surely. Our algorithm and analysis further develop the prior work of [Na et al., (2022)](/publication/pubs/na-2022-adaptive). Specifically, we allow nonlinear inequality constraints without requiring the strict complementary condition; refine some of the designs in [Na et al., (2022)](/publication/pubs/na-2022-adaptive) such as the feasibility error condition and the monotonically increasing sample size; strengthen the global convergence guarantee; and improve the sample complexity on the objective Hessian. We demonstrate the performance of the designed algorithm on a subset of nonlinear problems collected in CUTEst test set and on constrained logistic regression problems. ","tags":["math.OC","math.NA","cs.LG","cs.NA","stat.ML","recent"],"title":"Inequality Constrained Stochastic Nonlinear Optimization via Active-Set Sequential Quadratic Programming","type":"publication"},{"authors":[],"categories":[],"content":"The figure above is taken from Na et al., 2022, which illustrates the robustness of the optimal state-control solution to system perturbations on the two boundaries, referred to as exponential decay of sensitivity in Na and Anitescu, 2020.\nWe focus on solving time-varying nonlinear optimal control problems (OCPs) given by: $$\\begin{align} \\min_{\\boldsymbol{x}, \\boldsymbol{u}}\\;\\; \u0026amp; \\sum_{k=1}^{N-1} g_k(\\boldsymbol{x}_k, \\boldsymbol{u}_k) + g_N(\\boldsymbol{x}_N),\\\\ \\text{s.t.}\\;\\; \u0026amp; \\boldsymbol{x}_{k+1} = f_k(\\boldsymbol{x}_k, \\boldsymbol{u}_k),\\quad k = 0,\\ldots,N-1,\\\\ \u0026amp; \\boldsymbol{x}_{0} = \\bar{\\boldsymbol{x}}_0. \\end{align}$$ We are particularly interested in OCPs with a large number of stages $N$, which typically arise in settings with long horizons, fine time discretization resolutions, and multiple timescales. By conducting sensitivity analysis of OCPs, we propose offline distributed methods and online real-time model predictive control (MPC) methods, demonstrating promising global and local convergence guarantees.\nFor more details, please check out my highlighted works Na and Anitescu, 2020, 2023, Na, 2021, Na et al., 2021, Na et al., 2022, Shin et al., 2022!\n","date":1677715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677715200,"objectID":"ba14866a5ac4c2ff34fa10f253273c29","permalink":"https://senna1128.github.io/project/past/longoc/","publishdate":"2023-03-02T00:00:00Z","relpermalink":"/project/past/longoc/","section":"project","summary":"We study the *sensitivity* of optimal control policy to the system perturbation, and propose *offline distributed methods* and *online real-time MPC methods*.","tags":[],"title":"Long-horizon Nonlinear Optimal Control","type":"project"},{"authors":[],"categories":[],"content":"The figure above, taken from Na et al., 2020, displays the glass brains depicting the estimated differential network between the schizophrenia and control groups based on an fMRI dataset.\nSemiparametric models allow for flexible modeling of data without making rigid assumptions the parametric models make, while at the same time allowing for tractable estimation without suffering from the curse of dimensionality that obsesses fully nonparametric models. I have conducted research on semiparametric index models (Na and Kolar, 2021, Na et al., 2019) and semiparametric matrix completion problems (Na et al., 2020).\nAdditionally, I have worked on graphical models, which are utilized to capture complex relationships among observed variables and have applications in genetics, neuroscience, and computational biology. See Na et al., 2020 for the estimation of differential networks under the presence of latent factors.\n","date":1677628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677628800,"objectID":"0fa107518b1e899f503ca5eb49823c85","permalink":"https://senna1128.github.io/project/past/semigraphest/","publishdate":"2023-03-01T00:00:00Z","relpermalink":"/project/past/semigraphest/","section":"project","summary":"We study the estimation of various semiparametric and latent-variable graphical models in high dimensions, with applications to *genetics* and *neuroscience*.","tags":[],"title":"Semiparametric and Graphical Models","type":"project"},{"authors":["Sen Na","Mihai Anitescu"],"categories":[],"content":"","date":1677628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677651511,"objectID":"dbeb5bea5fd43a5f9e5a766cc7b05e12","permalink":"https://senna1128.github.io/publication/pubs/na-2023-superconvergence/","publishdate":"2023-03-01T06:18:31.261052Z","relpermalink":"/publication/pubs/na-2023-superconvergence/","section":"publication","summary":"We develop a one-Newton-step-per-horizon, online, lag-$L$, model predictive control (MPC) algorithm for solving discrete-time, equality-constrained, nonlinear dynamic programs. Based on recent sensitivity analysis results for the target problems class, we prove that the approach exhibits a behavior that we call superconvergence; that is, the tracking error with respect to the full-horizon solution is not only stable for successive horizon shifts, but also decreases with increasing shift order to a minimum value that decays exponentially in the length of the receding horizon. The key analytical step is the decomposition of the one-step error recursion of our algorithm into algorithmic error and perturbation error. We show that the perturbation error decays exponentially with the lag between two consecutive receding horizons, whereas the algorithmic error, determined by Newton's method, achieves quadratic convergence instead. Overall this approach induces our local exponential convergence result in terms of the receding horizon length for suitable values of $L$. Numerical experiments validate our theoretical findings.","tags":["math.DS","math.NA","cs.NA","cs.SY","eess.SY","recent"],"title":"Superconvergence of Online Optimization for Model Predictive Control","type":"publication"},{"authors":["Yuchen Fang","Sen Na","Michael W. Mahoney","Mladen Kolar"],"categories":null,"content":"","date":1677542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677542400,"objectID":"5cc96091cd016ec735c1ed29314c1f30","permalink":"https://senna1128.github.io/publication/ongoing/fang-2023-trust/","publishdate":"2023-02-28T00:00:00Z","relpermalink":"/publication/ongoing/fang-2023-trust/","section":"publication","summary":" We design a trust-region sequential quadratic programming (TR-SQP) method to find both **first- and second-order** stationary points for optimization problems with a stochastic objective and deterministic equality constraints. We name our method TR-SQP for STochastic Optimization with Random Models (TR-SQP-STORM). In each iteration, the algorithm constructs random models that require estimates of the objective value, gradient, and Hessian to satisfy adaptive accuracy conditions with a fixed probability. We introduce a novel reliability parameter, based on which we define reliable and unreliable iterations and adjust accuracy conditions accordingly. The reliability parameter equips the random models with extra flexibility to reduce the sample size at each step. To find first-order stationary points, we compute **gradient-steps** by employing the **adaptive relaxation technique** proposed by [Fang et al., 2022](/publication/preprints/fang-2022-fully). To find second-order stationary points, we design **eigen-steps** to explore the negative curvature of the reduced Lagrangian Hessian, with additional **second-order correctional steps** performed when necessary. Under reasonable assumptions, we establish both first- and second-order global convergence guarantees: with probability one, the TR-SQP-STORM iteration sequence converges to the first-order stationary point, with a subsequence converging to the second-order stationary point. We apply our method to a subset of problems in CUTEst set and on constrained Logistic regression problems to demonstrate its promising empirical performance.","tags":["math.OC","cs.NA","math.NA","stat.CO","stat.ML"],"title":"Trust-Region Sequential Quadratic Programming for Stochastic Optimization with Random Models","type":"publication"},{"authors":["Sen Na","Michał Dereziński","Michael W. Mahoney"],"categories":[],"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677622507,"objectID":"db0d159a1c5392a3e0aecbaad51f904f","permalink":"https://senna1128.github.io/publication/pubs/na-2022-hessian/","publishdate":"2023-02-28T22:15:07.889433Z","relpermalink":"/publication/pubs/na-2022-hessian/","section":"publication","summary":" We consider minimizing a smooth and strongly convex objective function using a stochastic Newton method. At each iteration, the algorithm is given an oracle access to a stochastic estimate of the Hessian matrix. The oracle model includes popular algorithms such as Subsampled Newton and Newton Sketch. Despite using second-order information, these existing methods do not exhibit superlinear convergence, unless the stochastic noise is gradually reduced to zero during the iteration, which would lead to a computational blow-up in the per-iteration cost. We propose to address this limitation with Hessian averaging: instead of using the most recent Hessian estimate, our algorithm maintains an average of all the past estimates. This reduces the stochastic noise while avoiding the computational blow-up. We show that this scheme exhibits local $Q$-superlinear convergence with a non-asymptotic rate of $(\\Upsilon\\sqrt{\\log t/t})^t$, where $\\Upsilon$ is proportional to the level of stochastic noise in the Hessian oracle. A potential drawback of this (uniform averaging) approach is that the averaged estimates contain Hessian information from the global phase of the method, i.e., before the iterates converge to a local neighborhood. This leads to a distortion that may substantially delay the superlinear convergence until long after the local neighborhood is reached. To address this drawback, we study a number of weighted averaging schemes that assign larger weights to recent Hessians, so that the superlinear convergence arises sooner, albeit with a slightly slower rate. Remarkably, we show that there exists a universal weighted averaging scheme that transitions to local convergence at an optimal stage, and still exhibits a superlinear convergence rate nearly (up to a logarithmic factor) matching that of uniform Hessian averaging. ","tags":["math.OC","cs.LG","stat.ML","recent"],"title":"Hessian Averaging in Stochastic Newton Methods Achieves Superlinear Convergence","type":"publication"},{"authors":["Sen Na","Sungho Shin","Mihai Anitescu","Victor M. Zavala"],"categories":[],"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677652142,"objectID":"4a48113d180700b02809f21e4b8c0b11","permalink":"https://senna1128.github.io/publication/pubs/na-2022-convergence/","publishdate":"2023-03-01T06:29:02.176835Z","relpermalink":"/publication/pubs/na-2022-convergence/","section":"publication","summary":"We study the convergence properties of an overlapping Schwarz decomposition algorithm for solving nonlinear optimal control problems (OCPs). The algorithm decomposes the time domain into a set of overlapping subdomains, and solves all subproblems defined over subdomains in parallel. The convergence is attained by updating primal-dual information at the boundaries of overlapping subdomains. We show that the algorithm exhibits local linear convergence, and that the convergence rate improves exponentially with the overlap size. We also establish global convergence results for a general quadratic programming, which enables the application of the Schwarz scheme inside second-order optimization algorithms (e.g., sequential quadratic programming). The theoretical foundation of our convergence analysis is a sensitivity result of nonlinear OCPs, which we call “exponential decay of sensitivity” (EDS). Intuitively, EDS states that the impact of perturbations at domain boundaries (i.e., initial and terminal time) on the solution decays exponentially as one moves into the domain. Here, we expand a previous analysis available in the literature by showing that EDS holds for both primal and dual solutions of nonlinear OCPs, under uniform second-order sufficient condition, controllability condition, and boundedness condition. We conduct experiments with a quadrotor motion planning problem and a partial differential equations (PDE) control problem to validate our theory, and show that the approach is significantly more efficient than alternating direction method of multipliers and as efficient as the centralized interior-point solver.","tags":["math.OC","math.DS","cs.LG","recent"],"title":"On the Convergence of Overlapping Schwarz Decomposition for Nonlinear Optimal Control","type":"publication"},{"authors":["Sungho Shin","Sen Na","Mihai Anitescu"],"categories":[],"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677622508,"objectID":"e6022d9735cfc2f8ec54f8b248fda6be","permalink":"https://senna1128.github.io/publication/preprints/shin-2022-optimal/","publishdate":"2023-02-28T22:15:08.368479Z","relpermalink":"/publication/preprints/shin-2022-optimal/","section":"publication","summary":"We study the performance of stochastic predictive control (SPC) for linear systems with a quadratic performance index and additive and multiplicative uncertainties. Under a finite support assumption, the problem can be cast as a finite-dimensional quadratic program, but the problem becomes quickly intractable as the problem size grows exponentially in the horizon length. SPC aims to compute approximate solutions by solving a sequence of problems with truncated prediction horizons and committing the solution in a receding-horizon fashion. While this approach is widely used in practice, its performance relative to the optimal solution is not well understood. This article reports for the first time a rigorous performance guarantee of SPC: under the standard stabilizability and detectability conditions, the dynamic regret of SPC is exponentially small in the prediction horizon length. Therefore, SPC can achieve near-optimal performance -- the expected performance can be made arbitrarily close to the optimal solution -- at a substantially reduced computational expense. ","tags":["cs.SY","eess.SY"],"title":"Near-Optimal Performance of Stochastic Predictive Control","type":"publication"},{"authors":["Mingyuan Ma","Sen Na","Xiaolu Zhang","Congzhou Chen","Jin Xu"],"categories":[],"content":"","date":1659312e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677706319,"objectID":"fe8c5e73ba18aecc5a5c24ade5fd125c","permalink":"https://senna1128.github.io/publication/pubs/ma-2022-sfgae/","publishdate":"2023-03-01T21:31:59.295013Z","relpermalink":"/publication/pubs/ma-2022-sfgae/","section":"publication","summary":"Increasing evidence has suggested that microRNAs (miRNAs) are important biomarkers of various diseases. Numerous graph neural network (GNN) models have been proposed for predicting miRNA–disease associations. However, the existing GNN-based methods have over-smoothing issue—the learned feature embeddings of miRNA nodes and disease nodes are indistinguishable when stacking multiple GNN layers. This issue makes the performance of the methods sensitive to the number of layers, and significantly hurts the performance when more layers are employed. In this study, we resolve this issue by a novel self-feature-based graph autoencoder model, shortened as SFGAE. The key novelty of SFGAE is to construct miRNA-self embeddings and disease-self embeddings, and let them be independent of graph interactions between two types of nodes. The novel self-feature embeddings enrich the information of typical aggregated feature embeddings, which aggregate the information from direct neighbors and hence heavily rely on graph interactions. SFGAE adopts a graph encoder with attention mechanism to concatenate aggregated feature embeddings and self-feature embeddings, and adopts a bilinear decoder to predict links. Our experiments show that SFGAE achieves state-of-the-art performance. In particular, SFGAE improves the average AUC upon recent [GAEMDA](https://doi.org/10.1093/bib/bbaa240) on the benchmark datasets HMDD v2.0 and HMDD v3.2, and consistently performs better when less (e.g. 10%) training samples are used. Furthermore, SFGAE effectively overcomes the over-smoothing issue and performs stably well on deeper models (e.g. eight layers). Finally, we carry out case studies on three human diseases, colon neoplasms, esophageal neoplasms and kidney neoplasms, and perform a survival analysis using kidney neoplasm as an example. The results suggest that SFGAE is a reliable tool for predicting potential miRNA–disease associations. ","tags":[],"title":"SFGAE: a Self-feature-based Graph Autoencoder Model for miRNA--disease Associations Prediction","type":"publication"},{"authors":["Sen Na","Mihai Anitescu","Mladen Kolar"],"categories":[],"content":"","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677622505,"objectID":"6e8ab71f82b65a126ad7098655bbfe3d","permalink":"https://senna1128.github.io/publication/pubs/na-2022-adaptive/","publishdate":"2023-02-28T22:15:05.404479Z","relpermalink":"/publication/pubs/na-2022-adaptive/","section":"publication","summary":"We consider solving nonlinear optimization problems with a stochastic objective and deterministic equality constraints. We assume for the objective that its evaluation, gradient, and Hessian are inaccessible, while one can compute their stochastic estimates by, for example, subsampling. We propose a stochastic algorithm based on sequential quadratic programming (SQP) that uses a differentiable exact augmented Lagrangian as the merit function. To motivate our algorithm design, we first revisit and simplify an old SQP method [(Lucidi S, 1990)](https://doi.org/10.1007/BF00940474) developed for solving deterministic problems, which serves as the skeleton of our stochastic algorithm. Based on the simplified deterministic algorithm, we then propose a non-adaptive SQP for dealing with stochastic objective, where the gradient and Hessian are replaced by stochastic estimates but the stepsizes are deterministic and prespecified. Finally, we incorporate a recent stochastic line search procedure [(Paquette and Scheinberg, 2020)](https://doi.org/10.1137/18M1216250) into the non-adaptive stochastic SQP to adaptively select the random stepsizes, which leads to an adaptive stochastic SQP. The global “almost sure” convergence for both non-adaptive and adaptive SQP methods is established. Numerical experiments on nonlinear problems in CUTEst test set demonstrate the superiority of the adaptive algorithm.","tags":["math.OC","math.NA","cs.NA","stat.CO","stat.ML","recent"],"title":"An Adaptive Stochastic Sequential Quadratic Programming with Differentiable Exact Augmented Lagrangians","type":"publication"},{"authors":["Sen Na","Michael W. Mahoney"],"categories":[],"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677622505,"objectID":"5b15bd4d108e00fca58fc1847f6e07a7","permalink":"https://senna1128.github.io/publication/preprints/na-2022-statistical/","publishdate":"2023-02-28T22:15:05.539693Z","relpermalink":"/publication/preprints/na-2022-statistical/","section":"publication","summary":" We consider statistical inference of equality-constrained stochastic nonlinear optimization problems. We develop a fully online stochastic sequential quadratic programming (StoSQP) method to solve the problems, which can be regarded as applying Newton’s method to the first-order optimality conditions (i.e., the KKT conditions). Motivated by recent designs of numerical second-order methods, we allow StoSQP to adaptively select any random stepsize $\\bar{\\alpha}_t$, as long as $\\beta_t≤\\bar{\\alpha}_t≤\\beta_t+\\chi_t$, for some control sequences $\\beta_t$ and $\\chi_t=o(\\beta_t)$. To reduce the dominant computational cost of second-order methods, we additionally allow StoSQP to inexactly solve quadratic programs via efficient randomized iterative solvers that utilize sketching techniques. Notably, we do not require the approximation error to diminish as iteration proceeds. For the developed method, we show that under mild assumptions (i) **computationally**, it can take at most **$O(1/\\epsilon^4)$** iterations (same as samples) to attain $\\epsilon$-stationarity; (ii) **statistically**, its primal-dual sequence $1/\\sqrt{\\beta_t}\\cdot (x_t−x^{\\star},\\lambda_t-\\lambda^\\star)$ converges to a mean-zero Gaussian distribution with a nontrivial covariance matrix depending on the underlying sketching distribution. Additionally, we establish the **almost-sure convergence rate** of the iterate $(x_t,\\lambda_t)$ along with the **Berry-Esseen bound**; the latter quantitatively measures the convergence rate of the distribution function. We analyze a **plug-in limiting covariance matrix estimator**, and demonstrate the performance of the method both on benchmark nonlinear problems in CUTEst test set and on linearly/nonlinearly constrained regression problems. ","tags":["math.OC","cs.LG","stat.ML"],"title":"Statistical Inference of Constrained Stochastic Optimization via Sketched Sequential Quadratic Programming","type":"publication"},{"authors":["Sen Na"],"categories":[],"content":"","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677622505,"objectID":"5c2f157762df5b6679b18d83c6e42839","permalink":"https://senna1128.github.io/publication/pubs/na-2021-global/","publishdate":"2023-02-28T22:15:05.710175Z","relpermalink":"/publication/pubs/na-2021-global/","section":"publication","summary":"We study a real-time iteration (RTI) scheme for solving online optimization problem appeared in nonlinear optimal control. The proposed RTI scheme modifies the existing RTI-based model predictive control (MPC) algorithm, by selecting the stepsize of each Newton step at each sampling time using a differentiable exact augmented Lagrangian. The scheme can adaptively select the penalty parameters of augmented Lagrangian on the fly, which are shown to be stabilized after certain time periods. We prove under generic assumptions that, by involving stepsize selection instead of always using a full Newton step (like what most of the existing RTIs do), the scheme converges globally: for any initial point, the KKT residuals of the subproblems converge to zero. A key step is to show that augmented Lagrangian keeps decreasing as horizon moves forward. We demonstrate the global convergence behavior of the proposed RTI scheme in a numerical experiment.","tags":["math.DS","math.NA","cs.NA","cs.SY","eess.SY","recent"],"title":"Global Convergence of Online Optimization for Nonlinear Model Predictive Control","type":"publication"},{"authors":["Mingyuan Ma","Sen Na","Hongyu Wang","Congzhou Chen","Jin Xu"],"categories":[],"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677622506,"objectID":"d90e79fe0654312546c558c626ed3cd5","permalink":"https://senna1128.github.io/publication/pubs/ma-2021-graph/","publishdate":"2023-02-28T22:15:06.146902Z","relpermalink":"/publication/pubs/ma-2021-graph/","section":"publication","summary":"Interactive news recommendation has been launched and attracted much attention recently. In this scenario, user’s behavior evolves from single click behavior to multiple behaviors including like, comment, share etc. However, most of the existing methods still use single click behavior as the unique criterion of judging user’s preferences. Further, although heterogeneous graphs have been applied in different areas, a proper way to construct a heterogeneous graph for interactive news data with an appropriate learning mechanism on it is still desired. To address the above concerns, we propose a graph-based behavior-aware network, which simultaneously considers six different types of behaviors as well as user’s demand on the news diversity. We have three mainsteps. First, we build an interaction behavior graph for multi-level and multi-category data. Second, we apply DeepWalk on the behavior graph to obtain entity semantics, then build a graph-based convolutional neural network called G-CNN to learn news representations, and an attention-based LSTM to learn behavior sequence representations. Third, we introduce core and coritivity features for the behavior graph, which measure the concentration degree of user’s interests. These features affect the trade-off between accuracy and diversity of our personalized recommendation system. Taking these features into account, our system finally achieves recommending news to different users at their different levels of concentration degrees.","tags":["cs.IR","cs.LG","stat.ML"],"title":"The Graph-Based Behavior-Aware Recommendation for Interactive News","type":"publication"},{"authors":["Sen Na","Mladen Kolar"],"categories":[],"content":"","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677622506,"objectID":"4b3dde0bd4591b7ac243331c4ce89160","permalink":"https://senna1128.github.io/publication/pubs/na-2021-high/","publishdate":"2023-02-28T22:15:06.348434Z","relpermalink":"/publication/pubs/na-2021-high/","section":"publication","summary":"We study the estimation of the parametric components of single and multiple index volatility models. Using the first- and second-order Stein’s identities, we develop methods that are applicable for the estimation of the variance index in the high-dimensional setting requiring finite moment condition, which allows for heavy-tailed data. Our approach complements the existing literature in the low-dimensional setting, while relaxing the conditions on estimation, and provides a novel approach in the high-dimensional setting. We prove that the statistical rate of convergence of our variance index estimators consists of a parametric rate and a nonparametric rate, where the latter appears from the estimation of the mean link function. However, under standard assumptions, the parametric rate dominates the rate of convergence and our results match the minimax optimal rate for the mean index estimation. Simulation results illustrate finite sample properties of our methodology and back our theoretical conclusions. ","tags":["math.ST","cs.LG","stat.ML","stat.TH","recent"],"title":"High-dimensional Index Volatility Models via Stein's Identity","type":"publication"},{"authors":["Mingyuan Ma","Sen Na","Hongyu Wang"],"categories":[],"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677622507,"objectID":"39c4c52c7fe9821306232067bee28228","permalink":"https://senna1128.github.io/publication/pubs/ma-2021-aegcn/","publishdate":"2023-02-28T22:15:06.94354Z","relpermalink":"/publication/pubs/ma-2021-aegcn/","section":"publication","summary":"We propose a novel neural network architecture, called autoencoder-constrained graph convolutional network, to solve node classification task on graph domains. As suggested by its name, the core of this model is a convolutional network operating directly on graphs, whose hidden layers are constrained by an autoencoder. Comparing with vanilla graph convolutional networks, the autoencoder step is added to reduce the information loss brought by Laplacian smoothing. We consider applying our model on both homogeneous graphs and heterogeneous graphs. For homogeneous graphs, the autoencoder approximates to the adjacency matrix of the input graph by taking hidden layer representations as encoder and another one-layer graph convolutional network as decoder. For heterogeneous graphs, since there are multiple adjacency matrices corresponding to different types of edges, the autoencoder approximates to the feature matrix of the input graph instead, and changes the encoder to a particularly designed multi-channel pre-processing network with two layers. In both cases, the error occurred in the autoencoder approximation goes to the penalty term in the loss function. In extensive experiments on citation networks and other heterogeneous graphs, we demonstrate that adding autoencoder constraints significantly improves the performance of graph convolutional networks. Further, we notice that our technique can be applied on graph attention network to improve the performance as well. This reveals the wide applicability of the proposed autoencoder technique.","tags":["cs.LG","stat.ML"],"title":"AEGCN: An Autoencoder-Constrained Graph Convolutional Network","type":"publication"},{"authors":["Sen Na","Mladen Kolar","Oluwasanmi Koyejo"],"categories":[],"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677622506,"objectID":"f462e89df31e8fcf1ba9d437f6f8485b","permalink":"https://senna1128.github.io/publication/pubs/na-2020-estimating/","publishdate":"2023-02-28T22:15:06.475576Z","relpermalink":"/publication/pubs/na-2020-estimating/","section":"publication","summary":"Differential graphical models are designed to represent the difference between the conditional dependence structures of two groups, and thus are of particular interest for scientific investigations. Motivated by modern applications, this manuscript considers an extended setting where each group is generated by a latent variable Gaussian graphical model. Due to the existence of latent factors, the differential network is decomposed into sparse and low-rank components, both of which are symmetric indefinite matrices. We estimate these two components simultaneously using a two-stage procedure: (i) an initialization stage, which computes a simple, consistent estimator, and (ii) a convergence stage, implemented using a projected alternating gradient descent algorithm applied to a nonconvex objective, initialized using the output of the first stage. We prove that given the initialization, the estimator converges linearly with a nontrivial, minimax optimal statistical error. Experiments on synthetic and real data illustrate that the proposed nonconvex procedure outperforms existing methods.","tags":["math.ST","stat.AP","stat.ME","stat.ML","stat.TH","recent"],"title":"Estimating Differential Latent Variable Graphical Models with Applications to Brain Connectivity","type":"publication"},{"authors":["Sen Na","Yuwei Luo","Zhuoran Yang","Zhaoran Wang","Mladen Kolar"],"categories":[],"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677622507,"objectID":"af8895b43efb6957c9d2ccc62ea4e649","permalink":"https://senna1128.github.io/publication/pubs/na-2020-semiparametric/","publishdate":"2023-02-28T22:15:07.062896Z","relpermalink":"/publication/pubs/na-2020-semiparametric/","section":"publication","summary":"Graph representation learning is a ubiquitous task in machine learning where the goal is to embed each vertex into a low-dimensional vector space. We consider the bipartite graph and formalize its representation learning problem as a statistical estimation problem of parameters in a semiparametric exponential family distribution: the bipartite graph is assumed to be generated by a semiparametric exponential family distribution, whose parametric component is given by the proximity of outputs of two one-layer neural networks that take high-dimensional features as inputs, while nonparametric (nuisance) component is the base measure. In this setting, the representation learning problem is equivalent to recovering the weight matrices, and the main challenges of estimation arise from the nonlinearity of activation functions and the nonparametric nuisance component of the distribution. To overcome these challenges, we propose a pseudo-likelihood objective based on the rank-order decomposition technique and show that the proposed objective is strongly convex in a neighborhood around the ground truth, so that a gradient descent-based method achieves linear convergence rate. Moreover, we prove that the sample complexity of the problem is linear in dimensions (up to logarithmic factors), which is consistent with parametric Gaussian models. However, our estimator is robust to any model misspecification within the exponential family, which is validated in extensive experiments.","tags":["stat.ML","cs.LG"],"title":"Semiparametric Nonlinear Bipartite Graph Representation Learning with Provable Guarantees","type":"publication"},{"authors":["Sen Na","Mihai Anitescu"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677622506,"objectID":"7b2ff304e1f4d27d88619ce1bb76c5e4","permalink":"https://senna1128.github.io/publication/pubs/na-2020-exponential/","publishdate":"2023-02-28T22:15:06.615033Z","relpermalink":"/publication/pubs/na-2020-exponential/","section":"publication","summary":"In this paper, we study the sensitivity of discrete-time dynamic programs with nonlinear dynamics and objective to perturbations in the initial conditions and reference parameters. Under uniform controllability and boundedness assumptions for the problem data, we prove that the directional derivative of the optimal state and control at time $k$, $x_k^\\star$ and $u_k^\\star$, with respect to the reference signal at time $i$, $d_i$, will have exponential decay in terms of $|k-i|$ with a decay rate $\\rho$ independent of the temporal horizon length. The key technical step is to prove that a version of the convexification approach proposed by [Verschueren et al., 2017](https://doi.org/10.1137/16m1081543) can be applied to the KKT conditions and results in a convex quadratic program with uniformly bounded data. In turn, Riccati techniques can be further employed to obtain the sensitivity result, borne from the observation that the directional derivatives are solutions of quadratic programs with structure similar to the KKT conditions themselves. We validate our findings with numerical experiments on a small nonlinear, nonconvex, dynamic program. ","tags":["math.NA","math.OC","eess.SY","recent"],"title":"Exponential Decay in the Sensitivity Analysis of Nonlinear Dynamic Programming","type":"publication"},{"authors":["Sen Na","Zhuoran Yang","Zhaoran Wang","Mladen Kolar"],"categories":[],"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677622506,"objectID":"e7810977cc89cfaca38bcf924f0a6b14","permalink":"https://senna1128.github.io/publication/pubs/na-2019-high/","publishdate":"2023-02-28T22:15:06.816791Z","relpermalink":"/publication/pubs/na-2019-high/","section":"publication","summary":"We study the parameter estimation problem for a varying index coefficient model in high dimensions. Unlike the most existing works that iteratively estimate the parameters and link functions, based on the generalized Stein's identity, we propose computationally efficient estimators for the high-dimensional parameters without estimating the link functions. We consider two different setups where we either estimate each sparse parameter vector individually or estimate the parameters simultaneously as a sparse or low-rank matrix. For all these cases, our estimators are shown to achieve optimal statistical rates of convergence (up to logarithmic terms in the low-rank setting). Moreover, throughout our analysis, we only require the covariate to satisfy certain moment conditions, which is significantly weaker than the Gaussian or elliptically symmetric assumptions that are commonly made in the existing literature. Finally, we conduct extensive numerical experiments to corroborate the theoretical results.","tags":["cs.LG","stat.ML","stat.CO","recent"],"title":"High-dimensional Varying Index Coefficient Models via Stein's Identity","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://senna1128.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3ef1c3ed755398dc4fffccfec12a9a68","permalink":"https://senna1128.github.io/misc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/misc/","section":"","summary":"miscellanea","tags":null,"title":"Misc.","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1553a3396ab8995de319b5acd3158d43","permalink":"https://senna1128.github.io/papers/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/papers/","section":"","summary":"List of my papers","tags":null,"title":"Papers","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f1d044c0738ab9f19347f15c290a71a1","permalink":"https://senna1128.github.io/research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/","section":"","summary":"research summary and projects","tags":null,"title":"Research","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0683f5804634b3020a9e3219fd1f3789","permalink":"https://senna1128.github.io/service/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/service/","section":"","summary":"List of my service","tags":null,"title":"Service","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"65de3680a280f6bf29dc34fe1adad5a6","permalink":"https://senna1128.github.io/talks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"List of my talks","tags":null,"title":"Talks","type":"widget_page"}]